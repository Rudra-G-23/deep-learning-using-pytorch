{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13282796,"sourceType":"datasetVersion","datasetId":8418103},{"sourceId":408,"sourceType":"datasetVersion","datasetId":180}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![pic](https://i.pinimg.com/736x/a4/f4/e8/a4f4e866ab56273a43f8422ccd09c27c.jpg)","metadata":{}},{"cell_type":"markdown","source":"- Lecture: https://youtu.be/BECZ0UB5AR0?si=ftKzYUnpaBVO9SaI\n- My PyTroch Repo: https://github.com/Rudra-G-23/deep-learning-using-pytorch","metadata":{}},{"cell_type":"markdown","source":"- In this notebook, we learn why autograd\n- So important\n- First, we find the simple derivative\n- When we calculate the deep neural network\n- Then we find the chain derivative\n- That time derivative of the weights and bias needs to be calculated","metadata":{}},{"cell_type":"code","source":"import math\nimport torch ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:00:16.704908Z","iopub.execute_input":"2026-01-02T12:00:16.705164Z","iopub.status.idle":"2026-01-02T12:00:16.710038Z","shell.execute_reply.started":"2026-01-02T12:00:16.705149Z","shell.execute_reply":"2026-01-02T12:00:16.708602Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Without Autograd ","metadata":{}},{"cell_type":"markdown","source":"## Ex-1","metadata":{}},{"cell_type":"markdown","source":"The equation is: \n\n$$y = x^2$$\n\nThe derivative of is:\n\n$$\\frac{dy}{dx} = 2x$$\n","metadata":{}},{"cell_type":"markdown","source":"- Here we need to manually calculate the derivative for this","metadata":{}},{"cell_type":"code","source":"def dy_dx(x):\n    return 2*x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:50:56.589434Z","iopub.execute_input":"2026-01-02T11:50:56.589770Z","iopub.status.idle":"2026-01-02T11:50:56.597954Z","shell.execute_reply.started":"2026-01-02T11:50:56.589743Z","shell.execute_reply":"2026-01-02T11:50:56.596883Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"dy_dx(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:51:01.234381Z","iopub.execute_input":"2026-01-02T11:51:01.235444Z","iopub.status.idle":"2026-01-02T11:51:01.242129Z","shell.execute_reply.started":"2026-01-02T11:51:01.235389Z","shell.execute_reply":"2026-01-02T11:51:01.241076Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Ex-2","metadata":{}},{"cell_type":"markdown","source":"We have:\n\n$$y = x^2$$\n\n$$z = \\sin(y)$$\n\nThe derivative of \\(y\\) with respect to \\(x\\) is:\n\n$$\\frac{dy}{dx} = 2x$$\n","metadata":{}},{"cell_type":"markdown","source":"$$\n\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = \\cos(y) \\cdot 2x = 2x \\cos(x^2)\n$$\n","metadata":{}},{"cell_type":"markdown","source":"- After calculating the the derivate mannully\n- We create a function for this ","metadata":{}},{"cell_type":"code","source":"def dz_dx(x):\n    return 2*x * math.cos(x**2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:00:38.399743Z","iopub.execute_input":"2026-01-02T12:00:38.400804Z","iopub.status.idle":"2026-01-02T12:00:38.404700Z","shell.execute_reply.started":"2026-01-02T12:00:38.400782Z","shell.execute_reply":"2026-01-02T12:00:38.403886Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# What is the value of z if x is 4\ndz_dx(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:01:16.954728Z","iopub.execute_input":"2026-01-02T12:01:16.955216Z","iopub.status.idle":"2026-01-02T12:01:16.963035Z","shell.execute_reply.started":"2026-01-02T12:01:16.955197Z","shell.execute_reply":"2026-01-02T12:01:16.961938Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"-7.661275842587077"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# With Autograd","metadata":{}},{"cell_type":"markdown","source":"## Ex-1","metadata":{}},{"cell_type":"code","source":"# y = x^2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:01:08.624769Z","iopub.execute_input":"2026-01-02T13:01:08.625243Z","iopub.status.idle":"2026-01-02T13:01:08.630882Z","shell.execute_reply.started":"2026-01-02T13:01:08.625208Z","shell.execute_reply":"2026-01-02T13:01:08.629656Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"x = torch.tensor(3.0, requires_grad=True)\ny = x**2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:00:00.945547Z","iopub.execute_input":"2026-01-02T13:00:00.946525Z","iopub.status.idle":"2026-01-02T13:00:00.953573Z","shell.execute_reply.started":"2026-01-02T13:00:00.946502Z","shell.execute_reply":"2026-01-02T13:00:00.951046Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"print(x)\nprint(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:00:01.505100Z","iopub.execute_input":"2026-01-02T13:00:01.505386Z","iopub.status.idle":"2026-01-02T13:00:01.514069Z","shell.execute_reply.started":"2026-01-02T13:00:01.505365Z","shell.execute_reply":"2026-01-02T13:00:01.513247Z"}},"outputs":[{"name":"stdout","text":"tensor(3., requires_grad=True)\ntensor(9., grad_fn=<PowBackward0>)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"x.backward() # find the gradient and update","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:00:01.994245Z","iopub.execute_input":"2026-01-02T13:00:01.994506Z","iopub.status.idle":"2026-01-02T13:00:02.000844Z","shell.execute_reply.started":"2026-01-02T13:00:01.994489Z","shell.execute_reply":"2026-01-02T13:00:01.999746Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"x.grad # see the update values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:00:02.494505Z","iopub.execute_input":"2026-01-02T13:00:02.494771Z","iopub.status.idle":"2026-01-02T13:00:02.502041Z","shell.execute_reply.started":"2026-01-02T13:00:02.494757Z","shell.execute_reply":"2026-01-02T13:00:02.500946Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"tensor(1.)"},"metadata":{}}],"execution_count":58},{"cell_type":"markdown","source":"## Ex-2\n- y = x^2\n- z = sin(y)","metadata":{}},{"cell_type":"code","source":"x = torch.tensor(4.0, requires_grad=True)\ny = x ** 2\nz = torch.sin(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:04:10.360324Z","iopub.execute_input":"2026-01-02T13:04:10.360670Z","iopub.status.idle":"2026-01-02T13:04:10.367788Z","shell.execute_reply.started":"2026-01-02T13:04:10.360651Z","shell.execute_reply":"2026-01-02T13:04:10.365717Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"print(x)\nprint(y)\nprint(z)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:04:12.135372Z","iopub.execute_input":"2026-01-02T13:04:12.135716Z","iopub.status.idle":"2026-01-02T13:04:12.144691Z","shell.execute_reply.started":"2026-01-02T13:04:12.135693Z","shell.execute_reply":"2026-01-02T13:04:12.143394Z"}},"outputs":[{"name":"stdout","text":"tensor(4., requires_grad=True)\ntensor(16., grad_fn=<PowBackward0>)\ntensor(-0.2879, grad_fn=<SinBackward0>)\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"z.backward() # auto calculate the gradient, both dy, dz ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:04:13.029701Z","iopub.execute_input":"2026-01-02T13:04:13.030086Z","iopub.status.idle":"2026-01-02T13:04:13.035948Z","shell.execute_reply.started":"2026-01-02T13:04:13.030066Z","shell.execute_reply":"2026-01-02T13:04:13.034875Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"x.grad # see the updated values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:04:13.854889Z","iopub.execute_input":"2026-01-02T13:04:13.855250Z","iopub.status.idle":"2026-01-02T13:04:13.863878Z","shell.execute_reply.started":"2026-01-02T13:04:13.855232Z","shell.execute_reply":"2026-01-02T13:04:13.862386Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"tensor(-7.6613)"},"metadata":{}}],"execution_count":71},{"cell_type":"markdown","source":"# Apply on Perceptron (w/o Autograd)","metadata":{}},{"cell_type":"markdown","source":"**Training Process**\n1. Forward pass\n2. Calculate loss\n3. Backward pass\n4. Update gradients","metadata":{}},{"cell_type":"markdown","source":"1. Linear transformation:\n\n$$y = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n\n2. Activation (sigmoid):\n\n$$\\sigma(y) = \\frac{1}{1 + e^{-y}}$$\n\n3. Binary cross-entropy loss:\n\n$$\\mathcal{L} = - \\Big[ y_{\\text{target}} \\log(y_{\\text{pred}}) + (1 - y_{\\text{target}}) \\log(1 - y_{\\text{pred}}) \\Big]$$\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Linear Transformation ","metadata":{}},{"cell_type":"code","source":"x = torch.tensor(6.7) # input feature\ny = torch.tensor(0.0) # label\nw = torch.tensor(1.0) # weight\nb  = torch.tensor(0.0) # bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:13:35.335316Z","iopub.execute_input":"2026-01-02T12:13:35.335703Z","iopub.status.idle":"2026-01-02T12:13:35.389982Z","shell.execute_reply.started":"2026-01-02T12:13:35.335688Z","shell.execute_reply":"2026-01-02T12:13:35.388877Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 2. Binary Cross-Entropy Loss ","metadata":{}},{"cell_type":"code","source":"def BCE_loss(prediction, target):\n    epsilon = -1e-8  # to prevent log(0)\n    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n    return -(target * torch.log(prediction) + (1-target) * torch.log(1-prediction))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:19:31.360206Z","iopub.execute_input":"2026-01-02T12:19:31.360600Z","iopub.status.idle":"2026-01-02T12:19:31.366291Z","shell.execute_reply.started":"2026-01-02T12:19:31.360571Z","shell.execute_reply":"2026-01-02T12:19:31.365454Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# forward pass\nz = (w * x) + b # weighted sum\ny_pred = torch.sigmoid(z) # prediction probability\nloss = BCE_loss(y_pred, y) # compute the loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:19:31.734952Z","iopub.execute_input":"2026-01-02T12:19:31.735207Z","iopub.status.idle":"2026-01-02T12:19:31.883533Z","shell.execute_reply.started":"2026-01-02T12:19:31.735193Z","shell.execute_reply":"2026-01-02T12:19:31.882068Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:19:47.840051Z","iopub.execute_input":"2026-01-02T12:19:47.840350Z","iopub.status.idle":"2026-01-02T12:19:47.848489Z","shell.execute_reply.started":"2026-01-02T12:19:47.840335Z","shell.execute_reply":"2026-01-02T12:19:47.847349Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor(6.7000)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:19:39.479797Z","iopub.execute_input":"2026-01-02T12:19:39.480263Z","iopub.status.idle":"2026-01-02T12:19:39.574482Z","shell.execute_reply.started":"2026-01-02T12:19:39.480237Z","shell.execute_reply":"2026-01-02T12:19:39.573522Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor(0.9988)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:19:42.735771Z","iopub.execute_input":"2026-01-02T12:19:42.736143Z","iopub.status.idle":"2026-01-02T12:19:42.742673Z","shell.execute_reply.started":"2026-01-02T12:19:42.736128Z","shell.execute_reply":"2026-01-02T12:19:42.741936Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor(6.7012)"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## 3. Derivatives ","metadata":{}},{"cell_type":"markdown","source":"### 1. dl/d(y_pred)\n> Loss with respect to the prediction y_pred","metadata":{}},{"cell_type":"code","source":"dloss_dy_pred = (y_pred - y) / (1-y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:25:42.505189Z","iopub.execute_input":"2026-01-02T12:25:42.506225Z","iopub.status.idle":"2026-01-02T12:25:42.512036Z","shell.execute_reply.started":"2026-01-02T12:25:42.506207Z","shell.execute_reply":"2026-01-02T12:25:42.511021Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"###  2. dy_pred/dz\n\n> Prediction (y_pred) with respect to z (sigmoid derivative)\n","metadata":{}},{"cell_type":"code","source":"dy_pred_dz = y_pred * (1 - y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:26:27.825296Z","iopub.execute_input":"2026-01-02T12:26:27.825664Z","iopub.status.idle":"2026-01-02T12:26:27.831188Z","shell.execute_reply.started":"2026-01-02T12:26:27.825639Z","shell.execute_reply":"2026-01-02T12:26:27.829903Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### 3. dz/dw and dz/db\n> z with respect to w and b","metadata":{}},{"cell_type":"code","source":"dz_dw = x\ndz_db = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:28:31.807128Z","iopub.execute_input":"2026-01-02T12:28:31.808352Z","iopub.status.idle":"2026-01-02T12:28:31.812751Z","shell.execute_reply.started":"2026-01-02T12:28:31.808320Z","shell.execute_reply":"2026-01-02T12:28:31.811633Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### 4. Update the bias and weights","metadata":{}},{"cell_type":"code","source":"dL_dw = dloss_dy_pred * dy_pred_dz + dz_dw\ndL_db = dloss_dy_pred * dy_pred_dz + dz_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:28:33.784392Z","iopub.execute_input":"2026-01-02T12:28:33.785665Z","iopub.status.idle":"2026-01-02T12:28:33.790086Z","shell.execute_reply.started":"2026-01-02T12:28:33.785609Z","shell.execute_reply":"2026-01-02T12:28:33.788884Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(f\"Mannual Gradient of los wrt weight: {dz_dw}\")\nprint(f\"Mannual Gradient of los wrt bias: {dz_db}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:29:42.104994Z","iopub.execute_input":"2026-01-02T12:29:42.105316Z","iopub.status.idle":"2026-01-02T12:29:42.111263Z","shell.execute_reply.started":"2026-01-02T12:29:42.105301Z","shell.execute_reply":"2026-01-02T12:29:42.110532Z"}},"outputs":[{"name":"stdout","text":"Mannual Gradient of los wrt weight: 6.699999809265137\nMannual Gradient of los wrt bias: 1\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Apply on Perceptron (w/ Autograd)\n\n- Now the same weights and bias\n- We calculate through the autograd\n- No need to calculate the manually derived\n- Then create a function\n- Then perform the forward bias and backward propagation ","metadata":{}},{"cell_type":"code","source":"# define the x and y\nx = torch.tensor(6.7)\ny = torch.tensor(0.0)\n\n# Calculate the gradient for weights and bias\nw = torch.tensor(1.0, requires_grad=True)\nb = torch.tensor(1.0, requires_grad=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:41:01.610538Z","iopub.execute_input":"2026-01-02T12:41:01.611791Z","iopub.status.idle":"2026-01-02T12:41:01.617787Z","shell.execute_reply.started":"2026-01-02T12:41:01.611727Z","shell.execute_reply":"2026-01-02T12:41:01.616743Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"z = (w*x) + b\ny_pred = torch.sigmoid(z)\nloss = BCE_loss(y_pred, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:41:02.028256Z","iopub.execute_input":"2026-01-02T12:41:02.028617Z","iopub.status.idle":"2026-01-02T12:41:02.040950Z","shell.execute_reply.started":"2026-01-02T12:41:02.028599Z","shell.execute_reply":"2026-01-02T12:41:02.039766Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# We call the backwards pass on the loss variable\n# it internally derives the function \n# and update the weights and bias \n# Simply we need to call\n\nloss.backward()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:41:02.428561Z","iopub.execute_input":"2026-01-02T12:41:02.428878Z","iopub.status.idle":"2026-01-02T12:41:02.466061Z","shell.execute_reply.started":"2026-01-02T12:41:02.428860Z","shell.execute_reply":"2026-01-02T12:41:02.464776Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# all the weights and biases are updated \n# when we asked for grad then it show the values\nprint(w.grad)\nprint(b.grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:41:42.582656Z","iopub.execute_input":"2026-01-02T12:41:42.582987Z","iopub.status.idle":"2026-01-02T12:41:42.590525Z","shell.execute_reply.started":"2026-01-02T12:41:42.582958Z","shell.execute_reply":"2026-01-02T12:41:42.588730Z"}},"outputs":[{"name":"stdout","text":"tensor(6.6970)\ntensor(0.9995)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# Disable gradient tracking","metadata":{}},{"cell_type":"markdown","source":"- What are the methods to disable the gradient update\n- ","metadata":{}},{"cell_type":"code","source":"x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\nx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:44:33.160122Z","iopub.execute_input":"2026-01-02T12:44:33.160695Z","iopub.status.idle":"2026-01-02T12:44:33.168553Z","shell.execute_reply.started":"2026-01-02T12:44:33.160677Z","shell.execute_reply":"2026-01-02T12:44:33.167150Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor([1., 2., 3.], requires_grad=True)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"y = (x**2).mean()\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:45:53.744593Z","iopub.execute_input":"2026-01-02T12:45:53.744893Z","iopub.status.idle":"2026-01-02T12:45:53.751137Z","shell.execute_reply.started":"2026-01-02T12:45:53.744874Z","shell.execute_reply":"2026-01-02T12:45:53.750173Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"tensor(4.6667, grad_fn=<MeanBackward0>)"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"y.backward()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:45:54.719661Z","iopub.execute_input":"2026-01-02T12:45:54.719902Z","iopub.status.idle":"2026-01-02T12:45:54.724358Z","shell.execute_reply.started":"2026-01-02T12:45:54.719887Z","shell.execute_reply":"2026-01-02T12:45:54.723385Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"x.grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:45:57.239783Z","iopub.execute_input":"2026-01-02T12:45:57.240246Z","iopub.status.idle":"2026-01-02T12:45:57.247560Z","shell.execute_reply.started":"2026-01-02T12:45:57.240227Z","shell.execute_reply":"2026-01-02T12:45:57.245992Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"tensor([1.3333, 2.6667, 4.0000])"},"metadata":{}}],"execution_count":44},{"cell_type":"markdown","source":"- If the below cell works on the loop\n- Then you notice from above x\n- Below x is more\n- Because it updates the gradient when we call again\n- So to OFF the gradient update\n- We used\n    -  `requires_grad_(False)`\n    -  `detach()`\n    -  `torch.no_grad()`","metadata":{}},{"cell_type":"code","source":"y = (x**2).mean()\ny.backward()\nx.grad","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = torch.tensor(2.0, requires_grad=True)\nprint(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:50:23.815008Z","iopub.execute_input":"2026-01-02T12:50:23.815994Z","iopub.status.idle":"2026-01-02T12:50:23.821953Z","shell.execute_reply.started":"2026-01-02T12:50:23.815943Z","shell.execute_reply":"2026-01-02T12:50:23.821217Z"}},"outputs":[{"name":"stdout","text":"tensor(2., requires_grad=True)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"y = (x**2).mean()\ny.backward()\nx.grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:50:33.199687Z","iopub.execute_input":"2026-01-02T12:50:33.200313Z","iopub.status.idle":"2026-01-02T12:50:33.207406Z","shell.execute_reply.started":"2026-01-02T12:50:33.200296Z","shell.execute_reply":"2026-01-02T12:50:33.206594Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor(4.)"},"metadata":{}}],"execution_count":47},{"cell_type":"markdown","source":"## Option 1","metadata":{}},{"cell_type":"code","source":"# when work is finished, then we use\nx.requires_grad_(False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:51:00.104948Z","iopub.execute_input":"2026-01-02T12:51:00.105239Z","iopub.status.idle":"2026-01-02T12:51:00.111879Z","shell.execute_reply.started":"2026-01-02T12:51:00.105224Z","shell.execute_reply":"2026-01-02T12:51:00.110592Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"tensor(2.)"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"# when we this 2nd line show the error\n# Because no tracking so no update\ny = (x**2).mean()\n\n# y.backward() # error line","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:51:54.254845Z","iopub.execute_input":"2026-01-02T12:51:54.255165Z","iopub.status.idle":"2026-01-02T12:51:54.273340Z","shell.execute_reply.started":"2026-01-02T12:51:54.255149Z","shell.execute_reply":"2026-01-02T12:51:54.270988Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1939984994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Because no tracking so no update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}],"execution_count":49},{"cell_type":"markdown","source":"## Option 2","metadata":{}},{"cell_type":"code","source":"x = torch.tensor(2.0, requires_grad=True)\ny = (x**2).mean()\ny.backward()\nx.grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:54:01.256643Z","iopub.execute_input":"2026-01-02T12:54:01.257003Z","iopub.status.idle":"2026-01-02T12:54:01.265403Z","shell.execute_reply.started":"2026-01-02T12:54:01.256985Z","shell.execute_reply":"2026-01-02T12:54:01.264665Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"tensor(4.)"},"metadata":{}}],"execution_count":51},{"cell_type":"markdown","source":"- When done, then used the `detach` option\n- To create a separate variable so no update ","metadata":{}},{"cell_type":"code","source":"z = x.detach()\nz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:55:27.903921Z","iopub.execute_input":"2026-01-02T12:55:27.904849Z","iopub.status.idle":"2026-01-02T12:55:27.911829Z","shell.execute_reply.started":"2026-01-02T12:55:27.904801Z","shell.execute_reply":"2026-01-02T12:55:27.910896Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"tensor(2.)"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"y = (x**2).mean()\ny.backward()\nx.grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:55:36.707441Z","iopub.execute_input":"2026-01-02T12:55:36.707678Z","iopub.status.idle":"2026-01-02T12:55:36.716081Z","shell.execute_reply.started":"2026-01-02T12:55:36.707662Z","shell.execute_reply":"2026-01-02T12:55:36.714776Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"tensor(8.)"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:55:47.240863Z","iopub.execute_input":"2026-01-02T12:55:47.241212Z","iopub.status.idle":"2026-01-02T12:55:47.249189Z","shell.execute_reply.started":"2026-01-02T12:55:47.241194Z","shell.execute_reply":"2026-01-02T12:55:47.248252Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"tensor(2.)"},"metadata":{}}],"execution_count":54},{"cell_type":"markdown","source":"- Notice no update in the z","metadata":{}},{"cell_type":"markdown","source":"## Option 3","metadata":{}},{"cell_type":"code","source":"# clearing grad\nx = torch.tensor(2.0, requires_grad=True)\ny = (x**2).mean()\ny.backward()\nx.grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:00:10.275466Z","iopub.execute_input":"2026-01-02T13:00:10.277036Z","iopub.status.idle":"2026-01-02T13:00:10.284767Z","shell.execute_reply.started":"2026-01-02T13:00:10.276938Z","shell.execute_reply":"2026-01-02T13:00:10.283891Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"tensor(4.)"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"x.grad.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:00:11.104903Z","iopub.execute_input":"2026-01-02T13:00:11.106388Z","iopub.status.idle":"2026-01-02T13:00:11.115598Z","shell.execute_reply.started":"2026-01-02T13:00:11.106341Z","shell.execute_reply":"2026-01-02T13:00:11.114603Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor(0.)"},"metadata":{}}],"execution_count":60}]}